# -*- coding: utf-8 -*-
"""adaptive_moe.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14qHB1nBWhWl13e8Vo3phH13sy3NgIBSg
"""

#Step1
import numpy as np
import torch
from torch.utils.data import TensorDataset, DataLoader
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_percentage_error
from openpyxl import Workbook, load_workbook
from datetime import datetime
import sys
import random

from google.colab import drive
drive.mount('/content/drive')



file1_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/83.csv'
file2_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/115.csv'
file3_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/67.csv'
file4_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/170.csv'
file5_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/9.csv'
file6_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/71.csv'
file7_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/88.csv'
file8_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/122.csv'

file9_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/10.csv'
file10_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/11.csv'
file11_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/21.csv'
file12_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/66.csv'

file13_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/84.csv'
file14_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/89.csv'
file15_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/116.csv'
file16_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/117.csv'

file17_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/123.csv'
file18_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/143.csv'
file19_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/171.csv'
file20_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/172.csv'
file21_path = '/content/drive/My Drive/MoE Project/Dataset/Data/MoE Data/173.csv'

#create dataframe and set index
df1 = pd.read_csv(file1_path)
# Define the string to remove
remove_string1 = '_83.npy'
remove_string2 = '_115.npy'
remove_string3 = '_67.npy'
remove_string4 = '_170.npy'
remove_string5 = '_9.npy'
remove_string6 = '_71.npy'
remove_string8 = '_122.npy'
remove_string9 = '_10.npy'
remove_string10 = '_11.npy'
remove_string11 = '_21.npy'
remove_string12 = '_66.npy'
remove_string13 = '_84.npy'
remove_string15 = '_116.npy'
remove_string16 = '_117.npy'
remove_string17 = '_123.npy'
remove_string18 = '_143.npy'
remove_string19 = '_171.npy'
remove_string20 = '_172.npy'
remove_string21 = '_173.npy'

#remove prefix from column header
def remove_prefix_1(col_name):
    return col_name.replace(remove_string1, '')

def remove_prefix_2(col_name):
    return col_name.replace(remove_string2, '')

def remove_prefix_3(col_name):
    return col_name.replace(remove_string3, '')

def remove_prefix_4(col_name):
    return col_name.replace(remove_string4, '')

def remove_prefix_5(col_name):
    return col_name.replace(remove_string5, '')

def remove_prefix_6(col_name):
    return col_name.replace(remove_string6, '')

def remove_prefix_8(col_name):
    return col_name.replace(remove_string8, '')

def remove_prefix_9(col_name):
    return col_name.replace(remove_string9, '')

def remove_prefix_10(col_name):
    return col_name.replace(remove_string10, '')

def remove_prefix_11(col_name):
    return col_name.replace(remove_string11, '')

def remove_prefix_12(col_name):
    return col_name.replace(remove_string12, '')

def remove_prefix_13(col_name):
    return col_name.replace(remove_string13, '')

def remove_prefix_15(col_name):
    return col_name.replace(remove_string15, '')

def remove_prefix_16(col_name):
    return col_name.replace(remove_string16, '')

def remove_prefix_17(col_name):
    return col_name.replace(remove_string17, '')

def remove_prefix_18(col_name):
    return col_name.replace(remove_string18, '')

def remove_prefix_19(col_name):
    return col_name.replace(remove_string19, '')

def remove_prefix_20(col_name):
    return col_name.replace(remove_string20, '')

def remove_prefix_21(col_name):
    return col_name.replace(remove_string21, '')

# Apply the function to rename columns
df1.rename(columns=remove_prefix_1, inplace=True)

df2 = pd.read_csv(file2_path)
df2.rename(columns=remove_prefix_2, inplace=True)

df3 = pd.read_csv(file3_path)
df3.rename(columns=remove_prefix_3, inplace=True)

df4 = pd.read_csv(file4_path)
df4.rename(columns=remove_prefix_4, inplace=True)

df5 = pd.read_csv(file5_path)
df5.rename(columns=remove_prefix_5, inplace=True)

df6 = pd.read_csv(file6_path)
df6.rename(columns=remove_prefix_6, inplace=True)

df8 = pd.read_csv(file8_path)
df8.rename(columns=remove_prefix_8, inplace=True)

df9 = pd.read_csv(file9_path)
df9.rename(columns=remove_prefix_9, inplace=True)

df10 = pd.read_csv(file10_path)
df10.rename(columns=remove_prefix_10, inplace=True)

df11 = pd.read_csv(file11_path)
df11.rename(columns=remove_prefix_11, inplace=True)

df12 = pd.read_csv(file12_path)
df12.rename(columns=remove_prefix_12, inplace=True)

df13 = pd.read_csv(file13_path)
df13.rename(columns=remove_prefix_13, inplace=True)

df15 = pd.read_csv(file15_path)
df15.rename(columns=remove_prefix_15, inplace=True)

df16 = pd.read_csv(file16_path)
df16.rename(columns=remove_prefix_16, inplace=True)

df17 = pd.read_csv(file17_path)
df17.rename(columns=remove_prefix_17, inplace=True)

df18 = pd.read_csv(file18_path)
df18.rename(columns=remove_prefix_18, inplace=True)

df19 = pd.read_csv(file19_path)
df19.rename(columns=remove_prefix_19, inplace=True)

df20 = pd.read_csv(file20_path)
df20.rename(columns=remove_prefix_20, inplace=True)

df21 = pd.read_csv(file21_path)
df21.rename(columns=remove_prefix_21, inplace=True)

#Data Normalization
scaler = MinMaxScaler(feature_range=(-1,1))

def min_max_scaler(df):
    return 2 * ((df - df.min()) / (df.max() - df.min())) - 1

def normalize_df1(df):
    numeric_cols = df.select_dtypes(include='number').iloc[:, 1:-1]
    df.iloc[:, 1:-1] = min_max_scaler(numeric_cols)
    return df

def normalize_df(df):
    numeric_cols = df.select_dtypes(include='number').columns
    df[numeric_cols] = min_max_scaler(df[numeric_cols])
    return df

# Apply the different normalization functions
df2 = normalize_df(df2.loc[:, df2.columns != 'Grid'])
df2 = df2.fillna(0)

df3 = normalize_df(df3.loc[:, df3.columns != 'Grid'])
df3 = df3.fillna(0)

df4 = normalize_df(df4.loc[:, df4.columns != 'Grid'])
df4 = df4.fillna(0)

df5 = normalize_df(df5.loc[:, df5.columns != 'Grid'])
df5 = df5.fillna(0)

df6 = normalize_df(df6.loc[:, df6.columns != 'Grid'])
df6 = df6.fillna(0)

df8 = normalize_df(df8.loc[:, df8.columns != 'Grid'])
df8 = df8.fillna(0)

df9 = normalize_df(df9.loc[:, df9.columns != 'Grid'])
df9 = df9.fillna(0)

df10 = normalize_df(df10.loc[:, df10.columns != 'Grid'])
df10 = df10.fillna(0)

df11 = normalize_df(df11.loc[:, df11.columns != 'Grid'])
df11 = df11.fillna(0)

df12 = normalize_df(df12.loc[:, df12.columns != 'Grid'])
df12 = df12.fillna(0)

df13 = normalize_df(df13.loc[:, df13.columns != 'Grid'])
df13 = df13.fillna(0)

df15 = normalize_df(df15.loc[:, df15.columns != 'Grid'])
df15 = df15.fillna(0)

df16 = normalize_df(df16.loc[:, df16.columns != 'Grid'])
df16 = df16.fillna(0)

df17 = normalize_df(df17.loc[:, df17.columns != 'Grid'])
df17 = df17.fillna(0)

df18 = normalize_df(df18.loc[:, df18.columns != 'Grid'])
df18 = df18.fillna(0)

df19 = normalize_df(df19.loc[:, df19.columns != 'Grid'])
df19 = df19.fillna(0)

df20 = normalize_df(df20.loc[:, df20.columns != 'Grid'])
df20 = df20.fillna(0)

#Step2
#-------------------------------------------------------
df=df1+df2+df3+df4+df5+df6+df8+df9+df10+df11+df12+df13+df15+df16+df17+df18+df19+df20
df = df.drop(columns=['Grid'])
df = normalize_df(df)

df.head()
df.fillna(0)

# Function to include headers as data, remove the first row, and transpose
def process_dataframe(df):
    df = pd.DataFrame([df.columns.tolist()] + df.values.tolist(), columns=None)
    df = df.iloc[1:]
    df = df.transpose()
    return df

# Apply the processing to each DataFrame
df = process_dataframe(df)
df.head()

print(df.shape)

# Sample data - replace 'df' with your actual DataFrame
data = df.values

# Calculate indices for train, validation, and test splits
train_size = int(len(data) * 0.7)
val_size = int(len(data) * 0.1)
test_size = len(data) - train_size - val_size

# Split the data manually without shuffling
train_data = data[:train_size,:]
val_data = data[train_size:train_size + val_size,:]
test_data = data[train_size + val_size:,:]

def create_dataset(data, window_size=10):
    X = []
    y = []
    for i in range(len(data) - window_size):
        X.append(data[i:i+window_size, :])
        y.append(data[i+window_size, :])
    X = np.array(X, dtype=np.float32)
    y = np.array(y, dtype=np.float32)
    return X, y

# Apply sliding window technique to each dataset
X_train, y_train = create_dataset(train_data)
X_val, y_val = create_dataset(val_data)
X_test, y_test = create_dataset(test_data)

# Convert to PyTorch tensors
X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
y_val_tensor = torch.tensor(y_val, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

# Create TensorDatasets
train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

# Create DataLoaders
batch_size = 32
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Optionally print shapes to confirm
print("Train shapes: X -", X_train_tensor.shape, "Y -", y_train_tensor.shape)
print("Validation shapes: X -", X_val_tensor.shape, "Y -", y_val_tensor.shape)
print("Test shapes: X -", X_test_tensor.shape, "Y -", y_test_tensor.shape)

#Step3
#MoE with custom loss and training complete dataset
#Train Experts to have distinct perspective

import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Define the MLP model
class MLP(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MLP, self).__init__()
        self.hidden1 = nn.Linear(input_size, hidden_sizes[0])
        self.hidden2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])
        self.hidden3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])
        self.output = nn.Linear(hidden_sizes[2], output_size)

    def forward(self, x):
        x = torch.relu(self.hidden1(x))
        x = torch.relu(self.hidden2(x))
        x = torch.relu(self.hidden3(x))
        x = self.output(x)
        return x

# Define the Mixture of Experts model
class MoE(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size, num_experts):
        super(MoE, self).__init__()
        self.experts = nn.ModuleList([MLP(input_size, hidden_sizes, output_size) for _ in range(num_experts)])
        self.gate = nn.Linear(input_size, num_experts)

    def forward(self, x, selected_experts):
        # Get gating weights
        gate_outputs = torch.softmax(self.gate(x), dim=1)  # Shape: (batch_size, num_experts)

        # Mask gate outputs to only use selected experts
        mask = torch.zeros_like(gate_outputs)
        mask[:, selected_experts] = gate_outputs[:, selected_experts]
        gate_outputs = mask

        # Normalize gate outputs so they sum to 1
        gate_outputs /= gate_outputs.sum(dim=1, keepdim=True) + 1e-8  # Avoid division by zero

        # Get outputs from all experts
        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)  # Shape: (batch_size, num_experts, output_size)

        # Weight the expert outputs by the gate outputs
        gated_expert_outputs = gate_outputs.unsqueeze(2) * expert_outputs  # Shape: (batch_size, num_experts, output_size)

        # Sum the weighted expert outputs
        output = gated_expert_outputs.sum(dim=1)  # Shape: (batch_size, output_size)

        return output

# Custom loss function with L1 and L2 regularization and weight difference
def custom_loss(output, target, experts):

    # Calculate L1 and L2 regularization
    l1_reg = sum(torch.sum(torch.abs(param)) for expert in experts for param in expert.parameters())
    l2_reg = sum(torch.sum(param ** 2) for expert in experts for param in expert.parameters())

    # Calculate ||W1 - W2||
    w1_params = list(experts[0].parameters())
    w2_params = list(experts[1].parameters())

    # Ensure that the two parameter lists are of the same length
    assert len(w1_params) == len(w2_params), "Parameter lists are not of the same length"

    # Calculate the norm of the difference
    weight_diff_norm = sum(torch.norm(p1 - p2) for p1, p2 in zip(w1_params, w2_params))

    # Final loss calculation
    total_loss = l1_reg + l2_reg - weight_diff_norm #maybe add mse_loss?

    return total_loss

# Initialize model parameters
input_size = 100000
hidden_sizes = [256, 128, 64]
output_size = 10000
num_experts = 16
num_epochs = 100
learning_rate = 0.001

# Create the MoE model
model = MoE(input_size, hidden_sizes, output_size, num_experts)

# Define an optimizer
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    selected_experts = random.sample(range(num_experts), 2)
    print(f'Epoch [{epoch+1}/{num_epochs}], Selected Experts: {selected_experts}')

    for batch_X, batch_y in train_dataloader:
        batch_X = batch_X.view(batch_X.size(0), -1).float()
        batch_y = batch_y.view(batch_y.size(0), -1).float()

        model.train()
        optimizer.zero_grad()

        outputs = model(batch_X, selected_experts)
        loss = custom_loss(outputs, batch_y, [model.experts[i] for i in selected_experts])
        loss.backward()

        # Zero gradients for all
        for param in model.parameters():
            if param.grad is not None:
                param.grad.zero_()

        # Retain gradients for selected experts
        for i in selected_experts:
            for param in model.experts[i].parameters():
                if param.grad is not None:
                    param.grad = param.grad  # Keeps current gradient

        # Retain gradients for gate parameters
        for param in model.gate.parameters():
            if param.grad is not None:
                param.grad = param.grad

        optimizer.step()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Save model
torch.save(model.state_dict(), 'moe_model.pth')

# Evaluation
model.eval()
predictions = []
actual_values = []

with torch.no_grad():
    for batch_X, batch_y in test_dataloader:
        batch_X = batch_X.view(batch_X.size(0), -1).float()
        batch_y = batch_y.view(batch_y.size(0), -1).float()

        outputs = model(batch_X, selected_experts)
        outputs = outputs[:, :batch_y.size(1)]

        predictions.extend(outputs.detach().cpu().numpy())
        actual_values.extend(batch_y.detach().cpu().numpy())

predictions = np.array(predictions)
actual_values = np.array(actual_values)

# Handle NaNs explicitly before evaluation
if np.isnan(predictions).any() or np.isnan(actual_values).any():
    print("Warning: NaNs found in predictions or actual values. Removing affected rows.")
    mask = ~np.isnan(predictions).any(axis=1) & ~np.isnan(actual_values).any(axis=1)
    predictions = predictions[mask]
    actual_values = actual_values[mask]

# Metrics
mae = mean_absolute_error(actual_values, predictions)
mse = mean_squared_error(actual_values, predictions)
rmse = np.sqrt(mse)
r2 = r2_score(actual_values, predictions)

def modified_mape(actual, pred, epsilon=1e-6):
    return np.mean(np.abs((actual - pred) / (actual + epsilon))) * 100

mape = modified_mape(actual_values, predictions)

print("Evaluation Results:")
print(f"MAE: {mae:.4f}")
print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")



#Step4 Load the pretrained model to learn the expertsâ€™ preferences.
#Data specialization
import pandas as pd
import numpy as np

df8 = df8.round(3)
df17 = df17.round(3)

df_test = df8+df17

data = df_test.iloc[:, 1:]


def min_max_scaling(df):
    min_val = df.min().min()
    max_val = df.max().max()
    scaled_df = 2 * (df - min_val) / (max_val - min_val) - 1
    return scaled_df

scaled_data = min_max_scaling(data)

# Create a list of DataFrames
df_list = [df1, df2, df3, df4, df5, df6, df8, df9, df10, df11, df12, df13, df15, df16, df17, df18, df19, df20]

#df1 = df1.drop('current', axis=1)
df1 = df1.drop('Grid', axis=1)


# Define feature group mapping
feature_groups = {
    "Momentum": [df5, df9, df10],
    "Temperature": [df6, df11],
    "Moisture": [df1, df3, df12, df13, df18],
    "Mass": [df8, df17],
    "Cloud": [df2, df15, df16],
    "Radiation": [df4, df19, df20],
}

def sum_dfs(dfs):
    # Check if the list of DataFrames is not empty
    if not dfs:
        raise ValueError("The list of DataFrames is empty")

    # Start with a DataFrame of zeros with the same shape as the first DataFrame
    sum_df = pd.DataFrame(0, index=dfs[0].index, columns=dfs[0].columns)

    # Add each DataFrame to the sum_df
    for df in dfs:
        sum_df += df

    # Fill any remaining NaNs with 0 (if needed)
    sum_df = sum_df.fillna(0)

    return sum_df

# Function to create partitions and normalize them
def sum_normalized_partitions(feature_groups):
    partitions = {}
    for group_name, dfs in feature_groups.items():
        # Sum and normalize DataFrames
        partition_data = sum_dfs(dfs)
        normalized_partition_data = min_max_scaling(partition_data)
        partitions[group_name] = normalized_partition_data
    return partitions

# Create and normalize partitions
partitions = sum_normalized_partitions(feature_groups)

from torch.utils.data import DataLoader, TensorDataset
import torch

# Function to include headers as data, remove the first row, and transpose
def process_dataframe(df):
    df = pd.DataFrame([df.columns.tolist()] + df.values.tolist(), columns=None)
    df = df.iloc[1:]
    df = df.transpose()
    return df

def create_dataset(data, window_size=10):
    X = []
    y = []
    for i in range(len(data) - window_size):
        X.append(data[i:i+window_size, :])
        y.append(data[i+window_size, :])
    X = np.array(X, dtype=np.float32)
    y = np.array(y, dtype=np.float32)
    return X, y

#Step4
#Train specialized dataset and the MoE Router
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from torch.utils.data import DataLoader, TensorDataset

# Define the MLP model
class MLP(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size):
        super(MLP, self).__init__()
        self.hidden1 = nn.Linear(input_size, hidden_sizes[0])
        self.hidden2 = nn.Linear(hidden_sizes[0], hidden_sizes[1])
        self.hidden3 = nn.Linear(hidden_sizes[1], hidden_sizes[2])
        self.output = nn.Linear(hidden_sizes[2], output_size)

    def forward(self, x):
        x = torch.relu(self.hidden1(x))
        x = torch.relu(self.hidden2(x))
        x = torch.relu(self.hidden3(x))
        x = self.output(x)
        return x

# Define the Mixture of Experts model
class MoE(nn.Module):
    def __init__(self, input_size, hidden_sizes, output_size, num_experts):
        super(MoE, self).__init__()
        self.experts = nn.ModuleList([MLP(input_size, hidden_sizes, output_size) for _ in range(num_experts)])
        self.gate = nn.Linear(input_size, num_experts)

    def forward(self, x):
        gate_outputs = torch.softmax(self.gate(x), dim=1)  # Shape: (batch_size, num_experts)
        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=1)  # Shape: (batch_size, num_experts, output_size)
        gated_expert_outputs = gate_outputs.unsqueeze(2) * expert_outputs  # Shape: (batch_size, num_experts, output_size)
        output = gated_expert_outputs.sum(dim=1)  # Shape: (batch_size, output_size)
        return output

# Custom loss function
def custom_loss(y_true, expert_predictions, router_weights):
    router_weights = torch.nan_to_num(router_weights, nan=0.0)
    expert_predictions = torch.nan_to_num(expert_predictions, nan=0.0)
    weighted_predictions = torch.sum(expert_predictions * router_weights.unsqueeze(2), dim=1)
    loss = nn.MSELoss()(weighted_predictions, y_true)
    return loss

# Function to aggregate predictions from multiple slices
def aggregate_predictions(predictions_list, weights=None):
    if weights is None:
        weights = [1.0] * len(predictions_list)
    aggregated_predictions = np.average(predictions_list, axis=0, weights=weights)
    return aggregated_predictions

# Load pre-trained expert weights
def load_pretrained_expert_weights(model, pretrained_model_path):
    pretrained_model = MoE(input_size, hidden_sizes, output_size, num_experts)
    pretrained_model.load_state_dict(torch.load(pretrained_model_path))
    model.experts = pretrained_model.experts  # Freeze the experts' weights
    for expert in model.experts:
        for param in expert.parameters():
            param.requires_grad = False
    return model

# Initialize model parameters
input_size = 100000
hidden_sizes = [256, 128, 64]
output_size = 10000
num_experts = 16
num_epochs = 20
learning_rate = 0.001
batch_size = 32

# Load pre-trained expert weights
pretrained_model_path = 'moe_model.pth'

# Initialize the MoE model
model = MoE(input_size, hidden_sizes, output_size, num_experts)

# Load pre-trained expert weights
model = load_pretrained_expert_weights(model, pretrained_model_path)

# Define an optimizer for router training
optimizer = optim.Adam(model.gate.parameters(), lr=learning_rate)

# Training loop for router training
for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0
    for partition_name, partition_df in partitions.items():
        # Assuming 'partition_df' is a DataFrame representing a partition
        # Normalize the partition DataFrame

        normalized_partition_df = normalize_df(partition_df)
        #print(normalized_partition_df.shape)

        # Process the DataFrame
        df_processed = process_dataframe(normalized_partition_df)
        # print(df_processed.shape)
        # Convert to PyTorch tensors
        data = df_processed.values

        #Calculate indices for train, validation, and test splits
        train_size = int(len(data) * 0.7)
        val_size = int(len(data) * 0.1)
        test_size = len(data) - train_size - val_size

        # Split the data manually without shuffling
        train_data = data[:train_size,:]
        val_data = data[train_size:train_size + val_size,:]
        test_data = data[train_size + val_size:,:]


        # Apply sliding window technique to each dataset
        X_train, y_train = create_dataset(train_data)
        X_val, y_val = create_dataset(val_data)
        X_test, y_test = create_dataset(test_data)

        # Convert to PyTorch tensors
        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
        y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
        y_val_tensor = torch.tensor(y_val, dtype=torch.float32)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

        # Create TensorDatasets
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

        # Create DataLoaders

        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        for batch_X, batch_y in train_dataloader:

            current_batch_size = batch_X.shape[0]
            batch_X = batch_X.reshape(current_batch_size, -1)
            #print(batch_X.shape)
            model.train()
            optimizer.zero_grad()
            outputs = model(batch_X)

            expert_predictions = torch.stack([expert(batch_X) for expert in model.experts], dim=1)
            router_weights = torch.softmax(model.gate(batch_X), dim=1)
            loss = custom_loss(batch_y, expert_predictions, router_weights)
            #print('loss', loss)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()

    avg_loss = total_loss / len(partitions)  # Average loss across all partitions
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')

# Evaluation phase
model.eval()
partition_predictions_lists = []
partition_actual_values_lists = []

with torch.no_grad():
    for partition_name, partition_df in partitions.items():
        # Assuming 'partition_df' is a DataFrame representing a partition
        # Normalize the partition DataFrame
        partition_predictions = []
        partition_actual_values = []

        normalized_partition_df = normalize_df(partition_df)

        # Process the DataFrame
        df_processed = process_dataframe(normalized_partition_df)
        # Convert to PyTorch tensors
        data = df_processed.values

        # Calculate indices for train, validation, and test splits
        train_size = int(len(data) * 0.7)
        val_size = int(len(data) * 0.1)
        test_size = len(data) - train_size - val_size

        # Split the data manually without shuffling
        train_data = data[:train_size, :]
        val_data = data[train_size:train_size + val_size, :]
        test_data = data[train_size + val_size:, :]

        # Apply sliding window technique to the test dataset
        X_test, y_test = create_dataset(test_data)

        # Convert to PyTorch tensors
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
        y_test_tensor = torch.tensor(y_test, dtype=torch.float32)

        # Create TensorDataset
        test_dataset = TensorDataset(X_test_tensor, y_test_tensor)

        # Create DataLoader
        test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        partition_predictions_list = []
        for batch_X, batch_y in test_dataloader:
            current_batch_size = batch_X.shape[0]
            batch_X = batch_X.reshape(current_batch_size, -1)
            outputs = model(batch_X)
            outputs = outputs.reshape(current_batch_size, -1)
            outputs = outputs[:, :batch_y.shape[1]]
            partition_predictions.extend(outputs.numpy())
            partition_actual_values.extend(batch_y.numpy())


        partition_predictions_lists.append(partition_predictions)
        partition_actual_values_lists.append(partition_actual_values)

aggregated_predictions = aggregate_predictions(partition_predictions_lists)
aggregated_actual_values = aggregate_predictions(partition_actual_values_lists)

aggregated_actual_values = np.nan_to_num(aggregated_actual_values, nan=0.0)
aggregated_predictions = np.nan_to_num(aggregated_predictions, nan=0.0)


print("Shape of actual_values:", aggregated_actual_values.shape)
print("Shape of aggregated_predictions:", aggregated_predictions.shape)

# Calculate evaluation metrics
mae = mean_absolute_error(aggregated_actual_values, aggregated_predictions)
mse = mean_squared_error(aggregated_actual_values, aggregated_predictions)
rmse = np.sqrt(mse)

print("Evaluation Results:")
print(f"MAE: {mae:.4f}")
print(f"MSE: {mse:.4f}")
print(f"RMSE: {rmse:.4f}")